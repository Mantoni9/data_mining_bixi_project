{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d13a369-99c5-4ab2-b8ab-b9bc4d58560b",
   "metadata": {},
   "source": [
    "# Model Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "370a2501-42c3-488b-9f8f-0f7baf8c8dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "# TODO: modell mit eurem ersetzen\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Empfohlen von sklearn als Alternative zu SVR, da besser skalierbar auf größere Datensätze\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "851d13cc-d1db-48f1-aa8e-af9ffa985376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "\n",
    "train_df = pd.read_csv('data/preprocessed_data/train.csv')\n",
    "val_df = pd.read_csv('data/preprocessed_data/validation.csv')\n",
    "test_df = pd.read_csv('data/preprocessed_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49d09b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop start date for regression (is represented as month, day, pm) \n",
    "# train_df = train_df.drop(columns='start_date')\n",
    "# test_df = test_df.drop(columns='start_date')\n",
    "\n",
    "# drop null values\n",
    "train_df = train_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdce9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "train_df = scaler.fit_transform(train_df) \n",
    "val_df = scaler.fit_transform(val_df) \n",
    "test_df = scaler.fit_transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c69e98f4-0254-418a-9214-deea6d4bad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "X_train = train_df.drop(columns='count')\n",
    "y_train = train_df['count']\n",
    "\n",
    "X_val = val_df.drop(columns='count')\n",
    "y_val = val_df['count']\n",
    "\n",
    "X_test = test_df.drop(columns='count')\n",
    "y_test = test_df['count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "baf1aa43-f423-4156-adf4-6e7b1a93db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 710964 entries, 0 to 723871\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count   Dtype  \n",
      "---  ------               --------------   -----  \n",
      " 0   latitude             710964 non-null  float64\n",
      " 1   longitude            710964 non-null  float64\n",
      " 2   distance_to_center   710964 non-null  float64\n",
      " 3   year                 710964 non-null  int64  \n",
      " 4   month                710964 non-null  int64  \n",
      " 5   weekday              710964 non-null  int64  \n",
      " 6   pm                   710964 non-null  int64  \n",
      " 7   holiday              710964 non-null  bool   \n",
      " 8   mean_temperature     710964 non-null  float64\n",
      " 9   total_precipitation  710964 non-null  float64\n",
      " 10  stations_count       710964 non-null  int64  \n",
      "dtypes: bool(1), float64(5), int64(5)\n",
      "memory usage: 60.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612aa94",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1a4193-d276-4a7a-9c73-1232ec73dd6d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "try:\n",
    "    # TODO: file-name ersetzen (in diesem File werden die Ergebnisse des Hyperparameter Tuning gespeichert)\n",
    "    hyperparameters_df = pd.read_csv('data/hyperparameter_tuning/SVR.csv')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # df containing hyperparameters and evaluation metrics of each run\n",
    "    hyperparameters_df = pd.DataFrame()\n",
    "    \n",
    "    # this function is used by optuna to tune the hyperparameters\n",
    "    def objective(trial):\n",
    "        # TODO: die Hyperparameter mit denen eures Modells ersetzen\n",
    "        # - integers: trial.suggest_int(name, low, high)\n",
    "        # - floats: trial.suggest_int(name, low, high)\n",
    "        # - kategorisch: trial.suggest_categorical(name, choices)\n",
    "        # (https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.trial.Trial.html)\n",
    "        # define hyperparameters\n",
    "        kernel = trial.suggest_categorical('kernel', ['poly', 'rbf'])\n",
    "        gamma = trial.suggest_categorical('gamma', ['scale', 'auto'])\n",
    "        c_regularizaion = trial.suggest_float('C', 1, 100)\n",
    "        \n",
    "        # TODO: mit eurem Modell ersetzen\n",
    "        # setup and train model\n",
    "        SVR_reg = SVR(\n",
    "            kernel=kernel,\n",
    "            gamma=gamma,\n",
    "            C=c_regularizaion\n",
    "        )\n",
    "        SVR_reg.fit(X_train, y_train)\n",
    "        \n",
    "        # make predictions\n",
    "        y_val_pred = SVR_reg.predict(X_val)\n",
    "        \n",
    "        # evaluate predictions\n",
    "        r_squared = r2_score(y_val, y_val_pred)\n",
    "        rmse = mean_squared_error(y_val, y_val_pred) ** 0.5\n",
    "        \n",
    "        # TODO: mit euren Hyperparametern ersetzen\n",
    "        # insert results in dataframe\n",
    "        global hyperparameters_df\n",
    "        hyperparameters_df = hyperparameters_df.append(\n",
    "            {'kernel': kernel,\n",
    "             'gamma': gamma,\n",
    "             'C': c_regularizaion,\n",
    "             'r_squared': r_squared,\n",
    "             'rmse': rmse},\n",
    "            ignore_index=True\n",
    "        )\n",
    "        \n",
    "        # return rmse -> optuna will optimize rmse\n",
    "        return rmse\n",
    "        \n",
    "        \n",
    "    study = optuna.create_study()\n",
    "    # start optimization\n",
    "    study.optimize(objective, n_trials=25)\n",
    "    \n",
    "    # TODO: evtl. müsst ihr auch noch mal die Datentypen anpassen\n",
    "    # convert to correct data types\n",
    "    # hyperparameters_df[['n_estimators', 'max_depth']] = hyperparameters_df[['n_estimators', 'max_depth']].astype('int')\n",
    "    \n",
    "    # sort hyperparameter tuning results and save file\n",
    "    hyperparameters_df = hyperparameters_df.sort_values('rmse', ascending=True)\n",
    "    hyperparameters_df = hyperparameters_df.reset_index(drop=True)\n",
    "    hyperparameters_df.to_csv('data/hyperparameter_tuning/SVR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1604c78-99a2-4ada-b6c2-3d46eadb92ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernel</th>\n",
       "      <th>gamma</th>\n",
       "      <th>C</th>\n",
       "      <th>r_squared</th>\n",
       "      <th>rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [kernel,  gamma,  C,  r_squared,  rmse]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be0ae7-5b16-49fd-be21-c212ae88169a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # final model evaluation\n",
    "\n",
    "# # TODO: mit eurem Modell und Hyperparametern ersetzen\n",
    "# # build and train model using the most successful hyperparameters\n",
    "# SVR_reg = SVR()\n",
    "# SVR_reg.fit(X_train, y_train)\n",
    "\n",
    "# # make predictions\n",
    "# y_test_pred = SVR_reg.predict(X_test)\n",
    "\n",
    "# # evaluate predictions\n",
    "# r_squared = r2_score(y_test, y_test_pred)\n",
    "# rmse = mean_squared_error(y_test, y_test_pred) ** 0.5\n",
    "\n",
    "# print(f'R^2:\\t{r_squared}')\n",
    "# print(f'RMSE:\\t{rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e528a22b",
   "metadata": {},
   "source": [
    "# LinearSVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f1622d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-11-17 19:44:06,731]\u001b[0m A new study created in memory with name: no-name-0280e7be-7cf4-4bdc-9048-5569b83d306e\u001b[0m\n",
      "c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "\u001b[33m[W 2022-11-17 19:48:00,529]\u001b[0m Trial 0 failed because of the following error: ValueError('Input X contains NaN.\\nLinearSVR does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values')\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\NicoDöring\\AppData\\Local\\Temp\\ipykernel_28568\\3349086273.py\", line 5, in <module>\n",
      "    hyperparameters_df = pd.read_csv('data/hyperparameter_tuning/linearSVR.csv')\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\util\\_decorators.py\", line 311, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 678, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 575, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 932, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1216, in _make_engine\n",
      "    self.handles = get_handle(  # type: ignore[call-overload]\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\common.py\", line 786, in get_handle\n",
      "    handle = open(\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/hyperparameter_tuning/linearSVR.csv'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 213, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\NicoDöring\\AppData\\Local\\Temp\\ipykernel_28568\\3349086273.py\", line 35, in objective\n",
      "    y_val_pred = LinearSVR_reg.predict(X_val)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 386, in predict\n",
      "    return self._decision_function(X)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\linear_model\\_base.py\", line 369, in _decision_function\n",
      "    X = self._validate_data(X, accept_sparse=[\"csr\", \"csc\", \"coo\"], reset=False)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\base.py\", line 577, in _validate_data\n",
      "    X = check_array(X, input_name=\"X\", **check_params)\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 899, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 146, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input X contains NaN.\n",
      "LinearSVR does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nLinearSVR does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[39m# TODO: file-name ersetzen (in diesem File werden die Ergebnisse des Hyperparameter Tuning gespeichert)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     hyperparameters_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/hyperparameter_tuning/linearSVR.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      7\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     \n\u001b[0;32m      9\u001b[0m     \u001b[39m# df containing hyperparameters and evaluation metrics of each run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m     f,\n\u001b[0;32m   1218\u001b[0m     mode,\n\u001b[0;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m     handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m         handle,\n\u001b[0;32m    788\u001b[0m         ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m         encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m         newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m     )\n\u001b[0;32m    793\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m     \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/hyperparameter_tuning/linearSVR.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study()\n\u001b[0;32m     58\u001b[0m \u001b[39m# start optimization\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m study\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m)\n\u001b[0;32m     61\u001b[0m \u001b[39m# TODO: evtl. müsst ihr auch noch mal die Datentypen anpassen\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39m# convert to correct data types\u001b[39;00m\n\u001b[0;32m     63\u001b[0m hyperparameters_df[\u001b[39m'\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m hyperparameters_df[\u001b[39m'\u001b[39m\u001b[39mepsilon\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\optuna\\study\\_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    263\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch):\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    265\u001b[0m \u001b[39mreturn\u001b[39;00m trial\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn [30], line 35\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     32\u001b[0m LinearSVR_reg\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     34\u001b[0m \u001b[39m# make predictions\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m y_val_pred \u001b[39m=\u001b[39m LinearSVR_reg\u001b[39m.\u001b[39;49mpredict(X_val)\n\u001b[0;32m     37\u001b[0m \u001b[39m# evaluate predictions\u001b[39;00m\n\u001b[0;32m     38\u001b[0m r_squared \u001b[39m=\u001b[39m r2_score(y_val, y_val_pred)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\linear_model\\_base.py:386\u001b[0m, in \u001b[0;36mLinearModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    373\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[39m    Predict using the linear model.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39m        Returns predicted values.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decision_function(X)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\linear_model\\_base.py:369\u001b[0m, in \u001b[0;36mLinearModel._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_decision_function\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    367\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 369\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, accept_sparse\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcoo\u001b[39;49m\u001b[39m\"\u001b[39;49m], reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    370\u001b[0m     \u001b[39mreturn\u001b[39;00m safe_sparse_dot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoef_\u001b[39m.\u001b[39mT, dense_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintercept_\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\base.py:577\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    575\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mValidation should be done on X, y or both.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    576\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 577\u001b[0m     X \u001b[39m=\u001b[39m check_array(X, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params)\n\u001b[0;32m    578\u001b[0m     out \u001b[39m=\u001b[39m X\n\u001b[0;32m    579\u001b[0m \u001b[39melif\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\utils\\validation.py:899\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    893\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    894\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mFound array with dim \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m expected <= 2.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    895\u001b[0m             \u001b[39m%\u001b[39m (array\u001b[39m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    896\u001b[0m         )\n\u001b[0;32m    898\u001b[0m     \u001b[39mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 899\u001b[0m         _assert_all_finite(\n\u001b[0;32m    900\u001b[0m             array,\n\u001b[0;32m    901\u001b[0m             input_name\u001b[39m=\u001b[39;49minput_name,\n\u001b[0;32m    902\u001b[0m             estimator_name\u001b[39m=\u001b[39;49mestimator_name,\n\u001b[0;32m    903\u001b[0m             allow_nan\u001b[39m=\u001b[39;49mforce_all_finite \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    904\u001b[0m         )\n\u001b[0;32m    906\u001b[0m \u001b[39mif\u001b[39;00m ensure_min_samples \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    907\u001b[0m     n_samples \u001b[39m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\sklearn\\utils\\validation.py:146\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    125\u001b[0m             \u001b[39mnot\u001b[39;00m allow_nan\n\u001b[0;32m    126\u001b[0m             \u001b[39mand\u001b[39;00m estimator_name\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[39m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    131\u001b[0m             \u001b[39m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    132\u001b[0m             msg_err \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[0;32m    133\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m does not accept missing values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m#estimators-that-handle-nan-values\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m             )\n\u001b[1;32m--> 146\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg_err)\n\u001b[0;32m    148\u001b[0m \u001b[39m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39melif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mdtype(\u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nLinearSVR does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "\n",
    "try:\n",
    "    # TODO: file-name ersetzen (in diesem File werden die Ergebnisse des Hyperparameter Tuning gespeichert)\n",
    "    hyperparameters_df = pd.read_csv('data/hyperparameter_tuning/linearSVR.csv')\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    \n",
    "    # df containing hyperparameters and evaluation metrics of each run\n",
    "    hyperparameters_df = pd.DataFrame()\n",
    "    \n",
    "    # this function is used by optuna to tune the hyperparameters\n",
    "    def objective(trial):\n",
    "        # TODO: die Hyperparameter mit denen eures Modells ersetzen\n",
    "        # - integers: trial.suggest_int(name, low, high)\n",
    "        # - floats: trial.suggest_int(name, low, high)\n",
    "        # - kategorisch: trial.suggest_categorical(name, choices)\n",
    "        # (https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.trial.Trial.html)\n",
    "        # define hyperparameters\n",
    "        epsilon = trial.suggest_float('epsilon', 0, 1, step=0.1)\n",
    "        loss = trial.suggest_categorical('loss', ['epsilon_insensitive', 'squared_epsilon_insensitive'])\n",
    "        c_regularizaion = trial.suggest_float('C', 1, 100)\n",
    "        \n",
    "        # TODO: mit eurem Modell ersetzen\n",
    "        # setup and train model\n",
    "        LinearSVR_reg = LinearSVR(\n",
    "            epsilon=epsilon,\n",
    "            loss=loss,\n",
    "            C=c_regularizaion,\n",
    "            random_state=1\n",
    "        )\n",
    "        LinearSVR_reg.fit(X_train, y_train)\n",
    "        \n",
    "        # make predictions\n",
    "        y_val_pred = LinearSVR_reg.predict(X_val)\n",
    "        \n",
    "        # evaluate predictions\n",
    "        r_squared = r2_score(y_val, y_val_pred)\n",
    "        rmse = mean_squared_error(y_val, y_val_pred) ** 0.5\n",
    "        \n",
    "        # TODO: mit euren Hyperparametern ersetzen\n",
    "        # insert results in dataframe\n",
    "        global hyperparameters_df\n",
    "        hyperparameters_df = hyperparameters_df.append(\n",
    "            { 'epsilon': epsilon,\n",
    "            'loss': loss,\n",
    "             'C': c_regularizaion,\n",
    "             'r_squared': r_squared,\n",
    "             'rmse': rmse},\n",
    "            ignore_index=True\n",
    "        )\n",
    "        \n",
    "        # return rmse -> optuna will optimize rmse\n",
    "        return rmse\n",
    "        \n",
    "        \n",
    "    study = optuna.create_study()\n",
    "    # start optimization\n",
    "    study.optimize(objective, n_trials=25)\n",
    "    \n",
    "    # TODO: evtl. müsst ihr auch noch mal die Datentypen anpassen\n",
    "    # convert to correct data types\n",
    "    hyperparameters_df['epsilon'] = hyperparameters_df['epsilon'].astype('float')\n",
    "    hyperparameters_df['C'] = hyperparameters_df['C'].astype('int')\n",
    "    \n",
    "    # sort hyperparameter tuning results and save file\n",
    "    hyperparameters_df = hyperparameters_df.sort_values('rmse', ascending=True)\n",
    "    hyperparameters_df = hyperparameters_df.reset_index(drop=True)\n",
    "    hyperparameters_df.to_csv('data/hyperparameter_tuning/linearSVR.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "607e8ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyperparameters_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ac8df42",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'epsilon'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3629\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'epsilon'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [21], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# final model evaluation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[39m# TODO: mit eurem Modell und Hyperparametern ersetzen\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m# build and train model using the most successful hyperparameters\u001b[39;00m\n\u001b[0;32m      5\u001b[0m LinearSVR_reg \u001b[39m=\u001b[39m LinearSVR(\n\u001b[1;32m----> 6\u001b[0m             epsilon\u001b[39m=\u001b[39mhyperparameters_df\u001b[39m.\u001b[39;49mloc[\u001b[39m0\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mepsilon\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m      7\u001b[0m             loss\u001b[39m=\u001b[39mhyperparameters_df\u001b[39m.\u001b[39mloc[\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      8\u001b[0m             C\u001b[39m=\u001b[39mhyperparameters_df\u001b[39m.\u001b[39mloc[\u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m             random_state\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m LinearSVR_reg\u001b[39m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     14\u001b[0m \u001b[39m# make predictions\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\core\\indexing.py:960\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    958\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(com\u001b[39m.\u001b[39mapply_if_callable(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m key)\n\u001b[0;32m    959\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m--> 960\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj\u001b[39m.\u001b[39;49m_get_value(\u001b[39m*\u001b[39;49mkey, takeable\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_takeable)\n\u001b[0;32m    961\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m    962\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m     \u001b[39m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\core\\frame.py:3615\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3612\u001b[0m     series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(col, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   3613\u001b[0m     \u001b[39mreturn\u001b[39;00m series\u001b[39m.\u001b[39m_values[index]\n\u001b[1;32m-> 3615\u001b[0m series \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_item_cache(col)\n\u001b[0;32m   3616\u001b[0m engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_engine\n\u001b[0;32m   3618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, MultiIndex):\n\u001b[0;32m   3619\u001b[0m     \u001b[39m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[0;32m   3620\u001b[0m     \u001b[39m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[0;32m   3621\u001b[0m     \u001b[39m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\core\\frame.py:3931\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   3926\u001b[0m res \u001b[39m=\u001b[39m cache\u001b[39m.\u001b[39mget(item)\n\u001b[0;32m   3927\u001b[0m \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3928\u001b[0m     \u001b[39m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[0;32m   3929\u001b[0m     \u001b[39m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m-> 3931\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(item)\n\u001b[0;32m   3932\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ixs(loc, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m   3934\u001b[0m     cache[item] \u001b[39m=\u001b[39m res\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3631\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3630\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3631\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3632\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3633\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3634\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3635\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3636\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'epsilon'"
     ]
    }
   ],
   "source": [
    "# final model evaluation\n",
    "\n",
    "# TODO: mit eurem Modell und Hyperparametern ersetzen\n",
    "# build and train model using the most successful hyperparameters\n",
    "LinearSVR_reg = LinearSVR(\n",
    "            epsilon=hyperparameters_df.loc[0, 'epsilon'],\n",
    "            loss=hyperparameters_df.loc[0, 'loss'],\n",
    "            C=hyperparameters_df.loc[0, 'C'],\n",
    "            random_state=1\n",
    ")\n",
    "\n",
    "LinearSVR_reg.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_test_pred = LinearSVR_reg.predict(X_test)\n",
    "\n",
    "# evaluate predictions\n",
    "r_squared = r2_score(y_test, y_test_pred)\n",
    "rmse = mean_squared_error(y_test, y_test_pred) ** 0.5\n",
    "\n",
    "print(f'R^2:\\t{r_squared}')\n",
    "print(f'RMSE:\\t{rmse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('data-mining-bixi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "96e673fa94a25c38f19d506dea23d01ecc2204c15434feb33c96a3bd0d805035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
