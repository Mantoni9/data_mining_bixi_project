{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b28efc-aa18-4710-b56e-c49a2d970955",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03f0aab-5a38-40f1-a9d7-a6c56fc778ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import cos, sin, arcsin, sqrt\n",
    "from math import radians\n",
    "from datetime import date\n",
    "import holidays\n",
    "import requests\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6069b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess(rides_file_path, stations_file_path):\n",
    "    '''\n",
    "    This function is used to read a rides file and all other necessary files and do the preprocessing.\n",
    "    \n",
    "    :param str rides_file_path: path to the rides file\n",
    "    :param str stations_file_path: path to the stations file\n",
    "    :return pd.DataFrame: dataframe ready to be used in the modelling process\n",
    "    '''\n",
    "    \n",
    "    rides_df = pd.read_csv(rides_file_path, parse_dates=[0, 2])\n",
    "    stations_df = pd.read_csv(stations_file_path)\n",
    "    \n",
    "    # turn all column names to lower case\n",
    "    rides_df.columns = rides_df.columns.str.lower()\n",
    "    stations_df.columns = stations_df.columns.str.lower()\n",
    "    \n",
    "    # this happens in file of 2021\n",
    "    if not 'start_station_code' in rides_df.columns:\n",
    "        rides_df = rides_df.rename(columns={'emplacement_pk_start': 'start_station_code',\n",
    "                                            'emplacement_pk_end': 'end_station_code'})\n",
    "    \n",
    "    # this happens in file of August 2019, because of invalid station codes\n",
    "    if rides_df['start_station_code'].dtype != 'int':\n",
    "        \n",
    "        # add column for integer values, insert None when a value can not be converted\n",
    "        def to_int_or_none(val):\n",
    "            try:\n",
    "                return(int(val))\n",
    "            except ValueError:\n",
    "                return None\n",
    "    \n",
    "        rides_df['start_station_code_int'] = rides_df['start_station_code'].apply(to_int_or_none)\n",
    "        rides_df['end_station_code_int'] = rides_df['end_station_code'].apply(to_int_or_none)\n",
    "    \n",
    "        # drop every row where station codes could not be converted to integer\n",
    "        rides_df = rides_df.dropna()\n",
    "        rides_df['start_station_code'] = rides_df['start_station_code_int'].astype('int')\n",
    "        rides_df['end_station_code'] = rides_df['end_station_code_int'].astype('int')\n",
    "        rides_df = rides_df.drop(columns=['start_station_code_int', 'end_station_code_int'])\n",
    "    \n",
    "    # aggregate rides: sum up rides between 0:00 to 12:00 and 12:00 to 0:00\n",
    "    ride_counts_df = rides_df.groupby([pd.Grouper(key='start_date', freq='12h'), 'start_station_code'])['end_date'].count()\n",
    "    ride_counts_df = ride_counts_df.to_frame()\n",
    "    ride_counts_df = ride_counts_df.rename(columns={'end_date': 'count'})\n",
    "    ride_counts_df = ride_counts_df.reset_index()\n",
    "    \n",
    "    # add am/pm flags (am = 0, pm = 1)\n",
    "    ride_counts_df['pm'] = ride_counts_df['start_date'].dt.hour.map({0: 0, 12: 1})\n",
    "    \n",
    "    # join coordinates of stations\n",
    "    ride_counts_df = ride_counts_df.merge(\n",
    "        stations_df[['code', 'latitude', 'longitude', 'density']], # add density of stations in nearby area (1km radius)\n",
    "        left_on='start_station_code',\n",
    "        right_on='code',\n",
    "        # inner join removes any station not specified in the stations dataset\n",
    "        how='inner'\n",
    "    )\n",
    "    \n",
    "    # add year, month and weekday\n",
    "    ride_counts_df['year'] = ride_counts_df['start_date'].dt.year\n",
    "    ride_counts_df['month'] = ride_counts_df['start_date'].dt.month\n",
    "    ride_counts_df['weekday'] = ride_counts_df['start_date'].dt.weekday\n",
    "    \n",
    "    # add holiday flag\n",
    "    ca_qc_holidays = holidays.country_holidays('CA', subdiv='QC')\n",
    "    ride_counts_df['holiday'] = ride_counts_df['start_date'].isin(ca_qc_holidays)\n",
    "    \n",
    "    # add weather data\n",
    "    weather_df = pd.read_csv('data/Canadian_climate_history.csv', parse_dates=[0])\n",
    "    weather_df.columns = weather_df.columns.str.lower()\n",
    "    weather_df = weather_df[['local_date', 'mean_temperature_montreal', 'total_precipitation_montreal']]\n",
    "    weather_df = weather_df.rename(\n",
    "        columns={'mean_temperature_montreal': 'mean_temperature',\n",
    "                 'total_precipitation_montreal': 'total_precipitation'}\n",
    "    )\n",
    "    \n",
    "    #weather for 2020 and 2021\n",
    "    weather_2020_2021 = pd.read_csv('data/Weather_2020_2021.csv', parse_dates=[0])\n",
    "\n",
    "    weather_2020_2021.columns = weather_2020_2021.columns.str.lower()\n",
    "    weather_2020_2021 = weather_2020_2021[[\"date/time\", \"mean temp (°c)\", \"total precip (mm)\"]]\n",
    "\n",
    "    weather_2020_2021 = weather_2020_2021.rename(\n",
    "        columns={'date/time': 'local date',\n",
    "                 'mean temp (°c)': 'mean_temperature',\n",
    "                 'total precip (mm)': 'total_precipitation'}\n",
    "    )\n",
    "\n",
    "    # add date attribute to join on\n",
    "    ride_counts_df['date'] = pd.to_datetime(ride_counts_df['start_date'].dt.date)\n",
    "    ride_counts_df = ride_counts_df.merge(weather_df, left_on='date', right_on='local_date')\n",
    "    ride_counts_df = ride_counts_df.merge(weather_2020_2021, left_on='date', right_on='local_date')\n",
    "    \n",
    "    # add distance to city center\n",
    "    def haversine(row):\n",
    "        lon1 = -73.56878\n",
    "        lat1 = 45.50354\n",
    "        lon2 = row['longitude']\n",
    "        lat2 = row['latitude']\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * arcsin(sqrt(a)) \n",
    "        km = 6367 * c\n",
    "        return km\n",
    "\n",
    "    ride_counts_df['distance_to_center'] = ride_counts_df.apply(lambda row: haversine(row), axis=1)\n",
    "    \n",
    "    # add total number of stations in this year\n",
    "    stations_count = len(stations_df['code'].unique())\n",
    "    ride_counts_df['stations_count'] = stations_count\n",
    "    \n",
    "    # elevation\n",
    "\n",
    "    # USGS Elevation Point Query Service\n",
    "    url = r'https://nationalmap.gov/epqs/pqs.php?'\n",
    "\n",
    "    def elevation_function(df, lat_column, lon_column):\n",
    "        elevations = []\n",
    "        for lat, lon in zip(df[lat_column], df[lon_column]):\n",
    "\n",
    "            #define rest query params\n",
    "            params = {\n",
    "                'output': 'json',\n",
    "                'x': lon,\n",
    "                'y': lat,\n",
    "                'units': 'Meters'\n",
    "            }\n",
    "\n",
    "             # format query string and return query value\n",
    "            result = requests.get((url + urllib.parse.urlencode(params)))\n",
    "            elevations.append(result.json()['USGS_Elevation_Point_Query_Service']['Elevation_Query']['Elevation'])\n",
    "\n",
    "        df['elevation_meters'] = elevations\n",
    "    \n",
    "    elevation_function(ride_counts_df, 'latitude', 'longitude')\n",
    "    \n",
    "    # only keep these columns\n",
    "    ride_counts_df = ride_counts_df[[\n",
    "        'latitude', 'longitude', 'distance_to_center',\n",
    "        'year', 'month', 'weekday', 'pm', 'holiday',\n",
    "        'mean_temperature', 'total_precipitation',\n",
    "        'stations_count', 'elev_meters_meters', 'density',\n",
    "        'count'\n",
    "    ]]\n",
    "    \n",
    "    return ride_counts_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d9fd3f-c2ce-4e69-8fef-cfc292fdf182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'holidays' has no attribute 'country_holidays'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 4\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata/preprocessed_data/train.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      5\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m     f,\n\u001b[0;32m   1218\u001b[0m     mode,\n\u001b[0;32m   1219\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1220\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1221\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1223\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1224\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1225\u001b[0m )\n\u001b[0;32m   1226\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\data-mining-bixi\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m     \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m     handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m         handle,\n\u001b[0;32m    788\u001b[0m         ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m         encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m         newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m     )\n\u001b[0;32m    793\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m     \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/preprocessed_data/train.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [3], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m         rides_file_path \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m/OD_\u001b[39m\u001b[39m{\u001b[39;00myear\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mmonth\u001b[39m:\u001b[39;00m\u001b[39m02d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     11\u001b[0m         \u001b[39m# run preprocessing function and append df\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m         month_df \u001b[39m=\u001b[39m read_and_preprocess(rides_file_path, stations_file_path)\n\u001b[0;32m     13\u001b[0m         train_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([train_df, month_df])\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[39m# save file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [2], line 65\u001b[0m, in \u001b[0;36mread_and_preprocess\u001b[1;34m(rides_file_path, stations_file_path)\u001b[0m\n\u001b[0;32m     62\u001b[0m ride_counts_df[\u001b[39m'\u001b[39m\u001b[39mweekday\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ride_counts_df[\u001b[39m'\u001b[39m\u001b[39mstart_date\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdt\u001b[39m.\u001b[39mweekday\n\u001b[0;32m     64\u001b[0m \u001b[39m# add holiday flag\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m ca_qc_holidays \u001b[39m=\u001b[39m holidays\u001b[39m.\u001b[39;49mcountry_holidays(\u001b[39m'\u001b[39m\u001b[39mCA\u001b[39m\u001b[39m'\u001b[39m, subdiv\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mQC\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     66\u001b[0m ride_counts_df[\u001b[39m'\u001b[39m\u001b[39mholiday\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m ride_counts_df[\u001b[39m'\u001b[39m\u001b[39mstart_date\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39misin(ca_qc_holidays)\n\u001b[0;32m     68\u001b[0m \u001b[39m# add weather data\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'holidays' has no attribute 'country_holidays'"
     ]
    }
   ],
   "source": [
    "# 2014 & 2017 (training data)\n",
    "# try reading file first\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/train.csv')\n",
    "except FileNotFoundError:\n",
    "    train_df = pd.DataFrame()\n",
    "    for year in range(2014, 2018):\n",
    "        stations_file_path = f'data/stations_preprocessed/Stations_{year}.csv'\n",
    "        for month in range(4, 11):\n",
    "            rides_file_path = f'data/{year}/OD_{year}-{month:02d}.csv'\n",
    "            # run preprocessing function and append df\n",
    "            month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "            train_df = pd.concat([train_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    train_df.to_csv('data/preprocessed_data/train.csv', index=False)\n",
    "\n",
    "# 2018 (model validation data)\n",
    "# try reading file first\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/validation.csv')\n",
    "except FileNotFoundError:\n",
    "    val_df = pd.DataFrame()\n",
    "    stations_file_path = f'data/stations_preprocessed/Stations_2018.csv'\n",
    "    for month in range(4, 11):\n",
    "        rides_file_path = f'data/2018/OD_2018-{month:02d}.csv'\n",
    "        # run preprocessing function and append df\n",
    "        month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "        val_df = pd.concat([val_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    val_df.to_csv('data/preprocessed_data/validation.csv', index=False)\n",
    "    \n",
    "# 2019 (test data)\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/test.csv')\n",
    "except FileNotFoundError:\n",
    "    test_df = pd.DataFrame()\n",
    "    stations_file_path = f'data/stations_preprocessed/Stations_2019.csv'\n",
    "    for month in range(4, 11):\n",
    "        rides_file_path = f'data/2019/OD_2019-{month:02d}.csv'\n",
    "        # run preprocessing function and append df\n",
    "        month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "        test_df = pd.concat([test_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    test_df.to_csv('data/preprocessed_data/test.csv', index=False)\n",
    "    \n",
    "# 2020 & 2021 (Corona years)\n",
    "#try:\n",
    "#    pd.read_csv('data/preprocessed_data/corona.csv')\n",
    "#except FileNotFoundError:\n",
    "#    corona_df = pd.DataFrame()\n",
    "#    for year in range(2020, 2021):\n",
    "#        stations_file_path = f'data/{year}/Stations_{year}.csv'\n",
    "#        rides_file_path = f'data/{year}/OD_{year}.csv'\n",
    "#        for month in range(4, 11):\n",
    "#            rides_file_path = f'data/{year}/OD_{year}-{month:02d}.csv'\n",
    "#            # run preprocessing function and append df\n",
    "#            month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "#            corona_df = pd.concat([train_df, month_df]).reset_index(drop=True)\n",
    "#        # run preprocessing function and append df\n",
    "#        year_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "#        corona_df = pd.concat(corona_df, year_df).reset_index(drop=True)\n",
    "#    # save file\n",
    "#    corona_df.to_csv('data/preprocessed_data/corona.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b403d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('data-mining-bixi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "96e673fa94a25c38f19d506dea23d01ecc2204c15434feb33c96a3bd0d805035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
