{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3b28efc-aa18-4710-b56e-c49a2d970955",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03f0aab-5a38-40f1-a9d7-a6c56fc778ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import cos, sin, arcsin, sqrt\n",
    "from math import radians\n",
    "from datetime import date\n",
    "import holidays\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6069b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess(rides_file_path, stations_file_path):\n",
    "    '''\n",
    "    This function is used to read a rides file and all other necessary files and do the preprocessing.\n",
    "    \n",
    "    :param str rides_file_path: path to the rides file\n",
    "    :param str stations_file_path: path to the stations file\n",
    "    :return pd.DataFrame: dataframe ready to be used in the modelling process\n",
    "    '''\n",
    "    \n",
    "    rides_df = pd.read_csv(rides_file_path, parse_dates=[0, 2])\n",
    "    stations_df = pd.read_csv(stations_file_path)\n",
    "    \n",
    "    # turn all column names to lower case\n",
    "    rides_df.columns = rides_df.columns.str.lower()\n",
    "    stations_df.columns = stations_df.columns.str.lower()\n",
    "    \n",
    "    # this happens in file of 2021\n",
    "    if not 'start_station_code' in rides_df.columns:\n",
    "        rides_df = rides_df.rename(columns={'emplacement_pk_start': 'start_station_code',\n",
    "                                            'emplacement_pk_end': 'end_station_code'})\n",
    "    \n",
    "    # this happens in file of August 2019, because of invalid station codes\n",
    "    if rides_df['start_station_code'].dtype != 'int':\n",
    "        \n",
    "        # add column for integer values, insert None when a value can not be converted\n",
    "        def to_int_or_none(val):\n",
    "            try:\n",
    "                return(int(val))\n",
    "            except ValueError:\n",
    "                return None\n",
    "    \n",
    "        rides_df['start_station_code_int'] = rides_df['start_station_code'].apply(to_int_or_none)\n",
    "        rides_df['end_station_code_int'] = rides_df['end_station_code'].apply(to_int_or_none)\n",
    "    \n",
    "        # drop every row where station codes could not be converted to integer\n",
    "        rides_df = rides_df.dropna()\n",
    "        rides_df['start_station_code'] = rides_df['start_station_code_int'].astype('int')\n",
    "        rides_df['end_station_code'] = rides_df['end_station_code_int'].astype('int')\n",
    "        rides_df = rides_df.drop(columns=['start_station_code_int', 'end_station_code_int'])\n",
    "    \n",
    "    # aggregate rides: sum up rides between 0:00 to 12:00 and 12:00 to 0:00\n",
    "    ride_counts_df = rides_df.groupby([pd.Grouper(key='start_date', freq='12h'), 'start_station_code'])['end_date'].count()\n",
    "    ride_counts_df = ride_counts_df.to_frame()\n",
    "    ride_counts_df = ride_counts_df.rename(columns={'end_date': 'count'})\n",
    "    ride_counts_df = ride_counts_df.reset_index()\n",
    "    \n",
    "    # add am/pm flags (am = 0, pm = 1)\n",
    "    ride_counts_df['pm'] = ride_counts_df['start_date'].dt.hour.map({0: 0, 12: 1})\n",
    "    \n",
    "    # join coordinates, elevation and density (1 km radius) of stations\n",
    "    ride_counts_df = ride_counts_df.merge(\n",
    "        stations_df[['code', 'latitude', 'longitude', 'density', 'elevation_meters']],\n",
    "        left_on='start_station_code',\n",
    "        right_on='code',\n",
    "        # inner join removes any station not specified in the stations dataset\n",
    "        how='inner'\n",
    "    )\n",
    "    # add total number of stations in this year\n",
    "    stations_count = len(stations_df['code'].unique())\n",
    "    ride_counts_df['stations_count'] = stations_count\n",
    "    \n",
    "    # add year, month and weekday\n",
    "    ride_counts_df['year'] = ride_counts_df['start_date'].dt.year\n",
    "    ride_counts_df['month'] = ride_counts_df['start_date'].dt.month\n",
    "    ride_counts_df['weekday'] = ride_counts_df['start_date'].dt.weekday\n",
    "    \n",
    "    # add holiday flag\n",
    "    ca_qc_holidays = holidays.country_holidays('CA', subdiv='QC')\n",
    "    ride_counts_df['holiday'] = ride_counts_df['start_date'].apply(lambda d: d in ca_qc_holidays)\n",
    "    \n",
    "    # add weather data\n",
    "    if ride_counts_df.loc[1, 'year'] < 2020:\n",
    "        weather_df = pd.read_csv('data/Canadian_climate_history.csv', parse_dates=[0])\n",
    "        weather_df.columns = weather_df.columns.str.lower()\n",
    "        weather_df = weather_df[['local_date', 'mean_temperature_montreal', 'total_precipitation_montreal']]\n",
    "        weather_df = weather_df.rename(\n",
    "            columns={'mean_temperature_montreal': 'mean_temperature',\n",
    "                     'total_precipitation_montreal': 'total_precipitation'}\n",
    "        )\n",
    "    # weather for 2020 and 2021\n",
    "    else:\n",
    "        weather_df = pd.read_csv('data/Weather_2020_2021.csv', parse_dates=[4])\n",
    "        weather_df.columns = weather_df.columns.str.lower()\n",
    "        weather_df = weather_df[[\"date/time\", \"mean temp (°c)\", \"total precip (mm)\"]]\n",
    "        weather_df = weather_df.rename(\n",
    "            columns={'date/time': 'local_date',\n",
    "                     'mean temp (°c)': 'mean_temperature',\n",
    "                     'total precip (mm)': 'total_precipitation'}\n",
    "        )\n",
    "    # interpolate missing temperature and precipitation values\n",
    "    weather_df[['mean_temperature', 'total_precipitation']] = weather_df[['mean_temperature', 'total_precipitation']].interpolate()\n",
    "    # add date attribute to join on and join\n",
    "    ride_counts_df['date'] = pd.to_datetime(ride_counts_df['start_date'].dt.date)\n",
    "    ride_counts_df = ride_counts_df.merge(weather_df, left_on='date', right_on='local_date')\n",
    "    \n",
    "    # add distance to city center\n",
    "    def haversine(row):\n",
    "        lon1 = -73.56878\n",
    "        lat1 = 45.50354\n",
    "        lon2 = row['longitude']\n",
    "        lat2 = row['latitude']\n",
    "        lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "        dlon = lon2 - lon1 \n",
    "        dlat = lat2 - lat1 \n",
    "        a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "        c = 2 * arcsin(sqrt(a)) \n",
    "        km = 6367 * c\n",
    "        return km\n",
    "\n",
    "    ride_counts_df['distance_to_center'] = ride_counts_df.apply(lambda row: haversine(row), axis=1)\n",
    "    \n",
    "    # only keep these columns\n",
    "    ride_counts_df = ride_counts_df[[\n",
    "        'latitude', 'longitude', 'distance_to_center',\n",
    "        'year', 'month', 'weekday', 'pm', 'holiday',\n",
    "        'mean_temperature', 'total_precipitation',\n",
    "        'stations_count', 'elevation_meters', 'density',\n",
    "        'count'\n",
    "    ]]\n",
    "    \n",
    "    return ride_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16408bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2014 - 2019 (all data)\n",
    "# try reading file first\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/all.csv')\n",
    "except FileNotFoundError:\n",
    "    train_df = pd.DataFrame()\n",
    "    for year in range(2014, 2019):\n",
    "        stations_file_path = f'data/stations_preprocessed/Stations_{year}.csv'\n",
    "        for month in range(4, 11):\n",
    "            rides_file_path = f'data/{year}/OD_{year}-{month:02d}.csv'\n",
    "            # run preprocessing function and append df\n",
    "            month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "            train_df = pd.concat([train_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    train_df.to_csv('data/preprocessed_data/all.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d9fd3f-c2ce-4e69-8fef-cfc292fdf182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2014 & 2017 (training data)\n",
    "# try reading file first\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/train.csv')\n",
    "except FileNotFoundError:\n",
    "    train_df = pd.DataFrame()\n",
    "    for year in range(2014, 2018):\n",
    "        stations_file_path = f'data/stations_preprocessed/Stations_{year}.csv'\n",
    "        for month in range(4, 11):\n",
    "            rides_file_path = f'data/{year}/OD_{year}-{month:02d}.csv'\n",
    "            # run preprocessing function and append df\n",
    "            month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "            train_df = pd.concat([train_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    train_df.to_csv('data/preprocessed_data/train.csv', index=False)\n",
    "\n",
    "# 2018 (model validation data)\n",
    "# try reading file first\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/validation.csv')\n",
    "except FileNotFoundError:\n",
    "    val_df = pd.DataFrame()\n",
    "    stations_file_path = f'data/stations_preprocessed/Stations_2018.csv'\n",
    "    for month in range(4, 11):\n",
    "        rides_file_path = f'data/2018/OD_2018-{month:02d}.csv'\n",
    "        # run preprocessing function and append df\n",
    "        month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "        val_df = pd.concat([val_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    val_df.to_csv('data/preprocessed_data/validation.csv', index=False)\n",
    "    \n",
    "# 2019 (test data)\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/test.csv')\n",
    "except FileNotFoundError:\n",
    "    test_df = pd.DataFrame()\n",
    "    stations_file_path = f'data/stations_preprocessed/Stations_2019.csv'\n",
    "    for month in range(4, 11):\n",
    "        rides_file_path = f'data/2019/OD_2019-{month:02d}.csv'\n",
    "        # run preprocessing function and append df\n",
    "        month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "        test_df = pd.concat([test_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    test_df.to_csv('data/preprocessed_data/test.csv', index=False)\n",
    "    \n",
    "# 2020 & 2021 (Corona years)\n",
    "try:\n",
    "    pd.read_csv('data/preprocessed_data/corona.csv')\n",
    "except FileNotFoundError:\n",
    "    corona_df = pd.DataFrame()\n",
    "    for year in range(2020, 2021): ### just 2020 for now ###\n",
    "        stations_file_path = f'data/stations_preprocessed/Stations_{year}.csv'\n",
    "        for month in range(4, 11):\n",
    "            rides_file_path = f'data/{year}/OD_{year}-{month:02d}.csv'\n",
    "            # run preprocessing function and append df\n",
    "            month_df = read_and_preprocess(rides_file_path, stations_file_path)\n",
    "            corona_df = pd.concat([corona_df, month_df]).reset_index(drop=True)\n",
    "    # save file\n",
    "    corona_df.to_csv('data/preprocessed_data/corona.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "183ba86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/preprocessed_data/train.csv')\n",
    "test = pd.read_csv('data/preprocessed_data/test.csv')\n",
    "validation = pd.read_csv('data/preprocessed_data/validation.csv')\n",
    "corona = pd.read_csv('data/preprocessed_data/corona.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5466f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdc05bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scaled = scaler.fit_transform(train)\n",
    "test_scaled = scaler.transform(test)\n",
    "validation_scaled = scaler.transform(validation)\n",
    "corona_scaled = scaler.transform(corona)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abf531ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(train_scaled, columns=train.columns)\n",
    "test = pd.DataFrame(test_scaled, columns=test.columns)\n",
    "validation = pd.DataFrame(validation_scaled, columns=validation.columns)\n",
    "corona = pd.DataFrame(corona_scaled, columns=corona.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "774fc6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('data/scaled_preprocessed_data/train.csv', index=False)\n",
    "test.to_csv('data/scaled_preprocessed_data/test.csv', index=False)\n",
    "validation.to_csv('data/scaled_preprocessed_data/validation.csv', index=False)\n",
    "corona.to_csv('data/scaled_preprocessed_data/corona.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('data-mining-bixi')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "96e673fa94a25c38f19d506dea23d01ecc2204c15434feb33c96a3bd0d805035"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
